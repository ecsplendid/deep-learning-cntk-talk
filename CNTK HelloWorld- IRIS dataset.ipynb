{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainData_cntk.txt - 120 items (40 each class) \n",
    "# testData_cntk.txt - remaining 30 items\n",
    "\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk import Trainer  # to train the NN\n",
    "from cntk.learners import sgd, learning_rate_schedule, \\\n",
    "  UnitType \n",
    "from cntk.ops import *  # input_variable() def\n",
    "from cntk.logging import ProgressPrinter \n",
    "from cntk.initializer import glorot_uniform \n",
    "from cntk.layers import default_options, Dense\n",
    "from cntk.io import CTFDeserializer, MinibatchSource, \\\n",
    "  StreamDef, StreamDefs, INFINITELY_REPEAT\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_print(arr, dec):\n",
    "  # print an array of float/double with dec decimals\n",
    "  fmt = \"%.\" + str(dec) + \"f\" # like %.4f\n",
    "  for i in range(0, len(arr)):\n",
    "    print(fmt % arr[i] + '  ', end='')\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training, input_dim, output_dim):\n",
    "  return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "    features = StreamDef(field='attribs', shape=input_dim,\n",
    "      is_sparse=False),\n",
    "    labels = StreamDef(field='species', shape=output_dim,\n",
    "      is_sparse=False)\n",
    "  )), randomize = is_training,\n",
    "    max_sweeps = INFINITELY_REPEAT if is_training else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_weights(fn, ihWeights, hBiases,\n",
    "  hoWeights, oBiases):\n",
    "  f = open(fn, 'w')\n",
    "  for vals in ihWeights:\n",
    "    for v in vals:\n",
    "      f.write(\"%s\\n\" % v)\n",
    "  for v in hBiases:\n",
    "    f.write(\"%s\\n\" % v)\n",
    "  for vals in hoWeights:\n",
    "    for v in vals:\n",
    "      f.write(\"%s\\n\" % v)\n",
    "  for v in oBiases:\n",
    "    f.write(\"%s\\n\" % v)\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this data look like then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\u0000r\u0000r\u0000o\u0000r\u0000:\u0000 \u00000\u0000x\u00008\u00000\u00000\u00007\u00000\u00000\u00005\u00007\u0000\r",
      "\u0000\r",
      "\u0000\n",
      "\u0000"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cat \"trainData_cntk.txt\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"iris-model.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of input_Var is: \n",
      "<class 'cntk.variables.Variable'>\n",
      "Creating a 4-2-3 tanh softmax NN for Iris data \n"
     ]
    }
   ],
   "source": [
    "# create NN, train, test, predict\n",
    "input_dim = 4\n",
    "hidden_dim = 2\n",
    "output_dim = 3\n",
    "\n",
    "train_file = \"trainData_cntk.txt\"\n",
    "test_file = \"testData_cntk.txt\"\n",
    "\n",
    "input_Var = C.ops.input(input_dim, np.float32)\n",
    "label_Var = C.ops.input(output_dim, np.float32)\n",
    "\n",
    "print(\"Type of input_Var is: \")\n",
    "print(type(input_Var))\n",
    "\n",
    "print(\"Creating a 4-2-3 tanh softmax NN for Iris data \") \n",
    "with default_options(init = glorot_uniform()):\n",
    "    hLayer = C.layers.Dense(hidden_dim, activation=C.ops.tanh, name='hidLayer')(input_Var)\n",
    "    oLayer = Dense(output_dim, activation=C.ops.softmax, name='outLayer')(hLayer)\n",
    "    nnet = oLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a cross entropy mini-batch Trainer \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Creating a cross entropy mini-batch Trainer \\n\")\n",
    "ce = C.cross_entropy_with_softmax(nnet, label_Var)\n",
    "pe = C.classification_error(nnet, label_Var)\n",
    "\n",
    "fixed_lr = 0.05\n",
    "lr_per_batch = learning_rate_schedule(fixed_lr,UnitType.minibatch)\n",
    "learner = C.sgd(nnet.parameters, lr_per_batch)\n",
    "trainer = C.Trainer(nnet, (ce, pe), [learner])\n",
    "\n",
    "max_iter = 5000  # 5000 maximum training iterations\n",
    "batch_size = 5   # mini-batch size  5\n",
    "progress_freq = 1000  # print error every n minibatches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many epochs are we doing? (120 is the number of training examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.666666666666664"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iter/120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a reader and input map, then start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "\n",
      " Minibatch[   1-1000]: loss = 1.108116 * 5000;\n",
      " Minibatch[1001-2000]: loss = 1.099623 * 5000;\n",
      " Minibatch[2001-3000]: loss = 0.972342 * 5000;\n",
      " Minibatch[3001-4000]: loss = 0.807777 * 5000;\n",
      " Minibatch[4001-5000]: loss = 0.768413 * 5000;\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "reader_train = create_reader(train_file, True, input_dim, output_dim)\n",
    "my_input_map = {\n",
    "input_Var : reader_train.streams.features,\n",
    "label_Var : reader_train.streams.labels\n",
    "}\n",
    "pp = ProgressPrinter(progress_freq)\n",
    "\n",
    "print(\"Starting training \\n\")\n",
    "for i in range(0, max_iter):\n",
    "    currBatch = reader_train.next_minibatch(batch_size, input_map = my_input_map)\n",
    "    trainer.train_minibatch(currBatch)\n",
    "    pp.update_with_trainer(trainer)\n",
    "print(\"\\nTraining complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating test data \n",
      "\n",
      "Classification error on the 30 test items = 0.066667\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating test data \\n\")\n",
    "reader_test = create_reader(test_file, False, input_dim, output_dim)\n",
    "\n",
    "numTestItems = 30\n",
    "\n",
    "allTest = reader_test.next_minibatch(numTestItems, input_map = my_input_map) \n",
    "test_error = trainer.test_minibatch(allTest)\n",
    "\n",
    "print(\"Classification error on the 30 test items = %f\" % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testData_cntk.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting Iris species for input features: \n",
      "6.9  3.1  4.6  1.3  \n",
      "\n",
      "Prediction is: \n",
      "0.263  0.682  0.055  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make a prediction for an unknown flower\n",
    "# first train versicolor = 7.0,3.2,4.7,1.4,0,1,0\n",
    "unknown = np.array([[6.9, 3.1, 4.6, 1.3]], dtype=np.float32) \n",
    "print(\"\\nPredicting Iris species for input features: \")\n",
    "my_print(unknown[0], 1)  # 1 decimal\n",
    "\n",
    "predicted = nnet.eval( {input_Var: unknown} ) \n",
    "print(\"Prediction is: \")\n",
    "my_print(predicted[0], 3)  # 3 decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained model input-to-hidden weights: \n",
      "\n",
      "[[ 0.61001551  0.92742485]\n",
      " [ 0.71527183  0.95194459]\n",
      " [-1.08553445 -0.52599716]\n",
      " [-1.06876552 -0.72444987]]\n",
      "\n",
      "Trained model hidden node biases: \n",
      "\n",
      "[ 0.14688115  0.03603337]\n",
      "\n",
      "Trained model hidden-to-output weights: \n",
      "\n",
      "[[ 3.22006011 -0.73119122 -4.19448519]\n",
      " [-0.85459208  0.35532627  0.0244552 ]]\n",
      "\n",
      "Trained model output node biases: \n",
      "\n",
      "[ 0.18597203  0.67358345 -0.8595528 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrained model input-to-hidden weights: \\n\")\n",
    "print(hLayer.hidLayer.W.value)\n",
    "print(\"\\nTrained model hidden node biases: \\n\")\n",
    "print(hLayer.hidLayer.b.value)\n",
    "\n",
    "print(\"\\nTrained model hidden-to-output weights: \\n\")\n",
    "print(oLayer.outLayer.W.value)\n",
    "print(\"\\nTrained model output node biases: \\n\")\n",
    "print(oLayer.outLayer.b.value)\n",
    "\n",
    "save_weights(\"weights.txt\", hLayer.hidLayer.W.value, \n",
    "    hLayer.hidLayer.b.value, oLayer.outLayer.W.value, \n",
    "    oLayer.outLayer.b.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
